{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36619,"status":"ok","timestamp":1754467337146,"user":{"displayName":"Deepali S Kunder","userId":"12762781305149853408"},"user_tz":-330},"id":"UldVzNpUIcnZ","outputId":"90215516-ed55-4d22-cbb0-a281639dc65d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzIuxBP5rqXu"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/ARShadowGAN_samples\n","!mkdir -p /content/drive/MyDrive/ARShadowGAN_checkpoints\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25240,"status":"ok","timestamp":1754467369931,"user":{"displayName":"Deepali S Kunder","userId":"12762781305149853408"},"user_tz":-330},"id":"hSVpteTCKTGD","outputId":"c41f50c0-bb00-492c-9ba6-1108bb88b182"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset unzipped to: /content/colab_data/shadow_ar_dataset\n"]}],"source":["# Path to your zipped dataset in Google Drive\n","zip_file_path = '/content/drive/MyDrive/colab_data/shadow_ar_dataset.zip' # Adjust if your path is different\n","\n","# Destination directory within Google Drive to unzip to\n","# This will create a new folder (e.g., 'shadow_ar_dataset') inside your colab_data folder\n","destination_path = '/content/colab_data/shadow_ar_dataset'\n","\n","# Create the destination directory if it doesn't exist (optional, unzip usually creates it)\n","import os\n","os.makedirs(destination_path, exist_ok=True)\n","\n","# Unzip the file\n","!unzip -q {zip_file_path} -d {destination_path}\n","print(f\"Dataset unzipped to: {destination_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1754467379602,"user":{"displayName":"Deepali S Kunder","userId":"12762781305149853408"},"user_tz":-330},"id":"D1l6Ic4dMXHV","outputId":"0214d25b-d43e-4ef8-cfb1-cdd2b53a90e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["mask: 2552 files\n","noshadow: 2552 files\n","robject: 2552 files\n","rshadow: 2552 files\n","shadow: 2552 files\n"]}],"source":["import os\n","\n","base_path = \"/content/colab_data/shadow_ar_dataset/dataset/train\"\n","\n","for folder in [\"mask\", \"noshadow\", \"robject\", \"rshadow\", \"shadow\"]:\n","    folder_path = os.path.join(base_path, folder)\n","    print(f\"{folder}: {len(os.listdir(folder_path))} files\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":567},"executionInfo":{"elapsed":11105,"status":"ok","timestamp":1754467394839,"user":{"displayName":"Deepali S Kunder","userId":"12762781305149853408"},"user_tz":-330},"id":"jRWJOMNurtwr","outputId":"4f8073b9-7c6c-4e3c-890d-3cc405fd2088"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.24.4\n","  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n","xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n","pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n","arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n","scipy 1.16.0 requires numpy<2.6,>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n","xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.24.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"48a66eaa342a4e5a85a35f1b8f9c0007"}},"metadata":{}}],"source":["pip install numpy==1.24.4 --force-reinstall\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RVMzZZbtq-y","outputId":"5518137a-5282-4ed2-998a-87b3a3c2d63b","executionInfo":{"status":"ok","timestamp":1754469782557,"user_tz":-330,"elapsed":2041505,"user":{"displayName":"Deepali S Kunder","userId":"12762781305149853408"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["✅ Loaded generator from /content/drive/MyDrive/generator_unet_epoch_581.pth\n","✅ Loaded discriminator from /content/drive/MyDrive/discriminator_epoch_581.pth\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [582/600]: 100%|██████████| 319/319 [02:02<00:00,  2.60it/s, D_Loss=0.334, G_Loss=5.22]\n","Epoch [583/600]: 100%|██████████| 319/319 [02:04<00:00,  2.56it/s, D_Loss=0.324, G_Loss=5.26]\n","Epoch [584/600]: 100%|██████████| 319/319 [02:04<00:00,  2.57it/s, D_Loss=0.32, G_Loss=5.37]\n","Epoch [585/600]: 100%|██████████| 319/319 [02:04<00:00,  2.56it/s, D_Loss=0.305, G_Loss=5.43]\n","Epoch [586/600]: 100%|██████████| 319/319 [02:03<00:00,  2.58it/s, D_Loss=0.32, G_Loss=5.28]\n","Epoch [587/600]: 100%|██████████| 319/319 [02:02<00:00,  2.60it/s, D_Loss=0.332, G_Loss=5.23]\n","Epoch [588/600]: 100%|██████████| 319/319 [02:03<00:00,  2.58it/s, D_Loss=0.316, G_Loss=5.37]\n","Epoch [589/600]: 100%|██████████| 319/319 [02:04<00:00,  2.56it/s, D_Loss=0.306, G_Loss=5.43]\n","Epoch [590/600]: 100%|██████████| 319/319 [02:03<00:00,  2.59it/s, D_Loss=0.306, G_Loss=5.3]\n","Epoch [591/600]: 100%|██████████| 319/319 [02:03<00:00,  2.59it/s, D_Loss=0.365, G_Loss=5.04]\n","Epoch [592/600]: 100%|██████████| 319/319 [02:02<00:00,  2.60it/s, D_Loss=0.315, G_Loss=5.26]\n","Epoch [593/600]: 100%|██████████| 319/319 [02:03<00:00,  2.59it/s, D_Loss=0.366, G_Loss=4.82]\n","Epoch [594/600]: 100%|██████████| 319/319 [02:03<00:00,  2.59it/s, D_Loss=0.345, G_Loss=4.96]\n","Epoch [595/600]: 100%|██████████| 319/319 [02:02<00:00,  2.60it/s, D_Loss=0.315, G_Loss=5.33]\n","Epoch [596/600]: 100%|██████████| 319/319 [02:02<00:00,  2.59it/s, D_Loss=0.32, G_Loss=5.16]\n","Epoch [597/600]: 100%|██████████| 319/319 [02:03<00:00,  2.58it/s, D_Loss=0.349, G_Loss=4.99]\n","Epoch [598/600]: 100%|██████████| 319/319 [02:03<00:00,  2.59it/s, D_Loss=0.312, G_Loss=5.14]\n","Epoch [599/600]: 100%|██████████| 319/319 [02:03<00:00,  2.59it/s, D_Loss=0.318, G_Loss=5.22]\n","Epoch [600/600]: 100%|██████████| 319/319 [02:02<00:00,  2.60it/s, D_Loss=0.292, G_Loss=5.37]\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from PIL import Image\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","# ==========================\n","# Dataset Loader\n","# ==========================\n","class ShadowDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.noshadow_dir = os.path.join(root_dir, 'noshadow')\n","        self.mask_dir = os.path.join(root_dir, 'mask')\n","        self.robject_dir = os.path.join(root_dir, 'robject')\n","        self.rshadow_dir = os.path.join(root_dir, 'rshadow')\n","        self.shadow_dir = os.path.join(root_dir, 'shadow')\n","        self.transform = transform\n","        self.filenames = os.listdir(self.noshadow_dir)\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        fname = self.filenames[idx]\n","\n","        def load_img(folder, mode='RGB'):\n","            path = os.path.join(folder, fname)\n","            image = Image.open(path).convert(mode)\n","            return self.transform(image)\n","\n","        noshadow = load_img(self.noshadow_dir, 'RGB')\n","        mask = load_img(self.mask_dir, 'L')\n","        robject = load_img(self.robject_dir, 'RGB')\n","        rshadow = load_img(self.rshadow_dir, 'RGB')\n","        shadow = load_img(self.shadow_dir, 'RGB')\n","\n","        # Concatenate: 3 + 1 + 3 + 3 = 10 channels\n","        return torch.cat([noshadow, mask, robject, rshadow], dim=0), shadow\n","\n","# ==========================\n","# UNet Generator\n","# ==========================\n","class UNetGenerator(nn.Module):\n","    def __init__(self, in_channels=10, out_channels=3, features=64):\n","        super().__init__()\n","\n","        def down_block(in_c, out_c):\n","            return nn.Sequential(\n","                nn.Conv2d(in_c, out_c, 4, stride=2, padding=1),\n","                nn.BatchNorm2d(out_c),\n","                nn.LeakyReLU(0.2)\n","            )\n","\n","        def up_block(in_c, out_c):\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(in_c, out_c, 4, stride=2, padding=1),\n","                nn.BatchNorm2d(out_c),\n","                nn.ReLU()\n","            )\n","\n","        self.down1 = down_block(in_channels, features)\n","        self.down2 = down_block(features, features * 2)\n","        self.down3 = down_block(features * 2, features * 4)\n","        self.down4 = down_block(features * 4, features * 8)\n","\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(features * 8, features * 8, 4, stride=2, padding=1),\n","            nn.ReLU()\n","        )\n","\n","        self.up1 = up_block(features * 8, features * 8)\n","        self.up2 = up_block(features * 8 * 2, features * 4)\n","        self.up3 = up_block(features * 4 * 2, features * 2)\n","        self.up4 = up_block(features * 2 * 2, features)\n","\n","        self.final = nn.Sequential(\n","            nn.ConvTranspose2d(features * 2, out_channels, 4, stride=2, padding=1),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        d1 = self.down1(x)\n","        d2 = self.down2(d1)\n","        d3 = self.down3(d2)\n","        d4 = self.down4(d3)\n","\n","        bottleneck = self.bottleneck(d4)\n","\n","        up1 = self.up1(bottleneck)\n","        up2 = self.up2(torch.cat([up1, d4], dim=1))\n","        up3 = self.up3(torch.cat([up2, d3], dim=1))\n","        up4 = self.up4(torch.cat([up3, d2], dim=1))\n","        return self.final(torch.cat([up4, d1], dim=1))\n","\n","# ==========================\n","# Discriminator\n","# ==========================\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(13, 64, 3, 1, 1), nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, 3, 1, 1), nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 1, 3, 1, 1), nn.Sigmoid()\n","        )\n","\n","    def forward(self, x, shadow):\n","        return self.model(torch.cat([x, shadow], dim=1))\n","\n","# ==========================\n","# Training Function\n","# ==========================\n","def train():\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.ToTensor()\n","    ])\n","\n","    dataset = ShadowDataset(\"/content/colab_data/shadow_ar_dataset/dataset/train\", transform)\n","    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n","\n","    generator = UNetGenerator().to(device)\n","    discriminator = Discriminator().to(device)\n","\n","    start_epoch = 581  # Resume from 60\n","    num_epochs = 600  # Train further\n","\n","    g_path = f\"/content/drive/MyDrive/generator_unet_epoch_{start_epoch}.pth\"\n","    d_path = f\"/content/drive/MyDrive/discriminator_epoch_{start_epoch}.pth\"\n","\n","    if os.path.exists(g_path):\n","        generator.load_state_dict(torch.load(g_path))\n","        print(f\"✅ Loaded generator from {g_path}\")\n","    if os.path.exists(d_path):\n","        discriminator.load_state_dict(torch.load(d_path))\n","        print(f\"✅ Loaded discriminator from {d_path}\")\n","\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    adversarial_loss = nn.BCELoss()\n","    pixelwise_loss = nn.L1Loss()\n","\n","    save_img_dir = \"/content/drive/MyDrive/shadow_outputs\"\n","    os.makedirs(save_img_dir, exist_ok=True)\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        g_total, d_total = 0, 0\n","        loop = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n","\n","        for i, (inputs, real_shadow) in enumerate(loop):\n","            inputs, real_shadow = inputs.to(device), real_shadow.to(device)\n","\n","            real_label = torch.ones((inputs.size(0), 1, 256, 256), device=device)\n","            fake_label = torch.zeros_like(real_label)\n","\n","            # === Train Discriminator ===\n","            optimizer_D.zero_grad()\n","            fake_shadow = generator(inputs).detach()\n","            d_real = discriminator(inputs, real_shadow)\n","            d_fake = discriminator(inputs, fake_shadow)\n","\n","            d_loss = 0.5 * (adversarial_loss(d_real, real_label) + adversarial_loss(d_fake, fake_label))\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            # === Train Generator ===\n","            optimizer_G.zero_grad()\n","            fake_shadow = generator(inputs)\n","            g_adv = adversarial_loss(discriminator(inputs, fake_shadow), real_label)\n","            g_l1 = pixelwise_loss(fake_shadow, real_shadow)\n","            g_loss = g_adv + 100 * g_l1  # Weighted sum\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","            g_total += g_loss.item()\n","            d_total += d_loss.item()\n","            loop.set_postfix(G_Loss=g_total / (i+1), D_Loss=d_total / (i+1))\n","\n","        # Save model\n","        torch.save(generator.state_dict(), f\"/content/drive/MyDrive/generator_unet_epoch_{epoch+1}.pth\")\n","        torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/discriminator_epoch_{epoch+1}.pth\")\n","\n","        # Save sample images\n","        save_image(fake_shadow[:4], f\"{save_img_dir}/fake_epoch_{epoch+1}.jpg\", normalize=True)\n","        save_image(real_shadow[:4], f\"{save_img_dir}/real_epoch_{epoch+1}.jpg\", normalize=True)\n","\n","if __name__ == \"__main__\":\n","    train()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"141UegFaTCsejES6WE3vbyUkA6DOjCNFi","timestamp":1752569524036}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}